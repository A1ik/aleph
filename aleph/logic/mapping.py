import logging
from banal import first
from followthemoney import model
from followthemoney.helpers import remove_checksums

from aleph.core import db, archive
from aleph.model import Mapping, Events
from aleph.queues import queue_task, OP_INDEX
from aleph.index.entities import get_entity
from aleph.index.collections import delete_entities
from aleph.logic.collections import update_collection
from aleph.logic.aggregator import get_aggregator, drop_aggregator
from aleph.logic.notifications import publish

log = logging.getLogger(__name__)


def _get_table_csv_link(table):
    properties = table.get('properties', {})
    csv_hash = first(properties.get('csvHash'))
    if csv_hash is None:
        raise RuntimeError("Source table doesn't have a CSV version")
    url = archive.generate_url(csv_hash)
    if url is None:
        local_path = archive.load_file(csv_hash)
        if local_path is not None:
            url = local_path.as_posix()
    if url is None:
        raise RuntimeError("Could not generate CSV URL for the table")
    return url


def mapping_origin(mapping_id):
    return 'mapping:%s' % mapping_id


def load_mapping(stage, collection, mapping_id):
    """Flush and reload all entities generated by a mapping."""
    flush_mapping(stage, collection, mapping_id, sync=True)
    mapping = Mapping.by_id(mapping_id)
    if mapping is None:
        return log.error("Could not find mapping: %s", mapping_id)
    publish(Events.LOAD_MAPPING,
            params={'collection': collection, 'table': mapping.table_id},
            channels=[collection, mapping.role],
            actor_id=mapping.role_id)
    aggregator = get_aggregator(collection)
    try:
        entities_count = 0
        entity_ids = set()
        for proxy in map_to_aggregator(collection, mapping, aggregator):
            entity_ids.add(proxy.id)
            entities_count += 1
            if len(entity_ids) > 500:
                payload = {'entity_ids': entity_ids}
                queue_task(collection, OP_INDEX,
                           job_id=stage.job.id,
                           payload=payload)
                entity_ids = set()
                log.info("[%s] Loaded %s entities...",
                         collection.foreign_id,
                         entities_count)

        if len(entity_ids):
            payload = {'entity_ids': entity_ids}
            queue_task(collection, OP_INDEX,
                       job_id=stage.job.id,
                       payload=payload)
        mapping.set_status(status=Mapping.SUCCESS)
        log.info("[%s] Mapping done (%s entities)",
                 mapping.id, entities_count)
    except Exception as exc:
        mapping.set_status(status=Mapping.FAILED, error=str(exc))
    finally:
        aggregator.close()


def map_to_aggregator(collection, mapping, aggregator):
    table = get_entity(mapping.table_id)
    if table is None:
        table = aggregator.get(mapping.table_id)
    if table is None:
        raise RuntimeError("Table cannot be found: %s" % mapping.table_id)
    config = {
        'csv_url': _get_table_csv_link(table),
        'entities': mapping.query
    }
    mapper = model.make_mapping(config, key_prefix=collection.foreign_id)
    origin = mapping_origin(mapping.id)
    aggregator.delete(origin=origin)
    writer = aggregator.bulk()
    for idx, record in enumerate(mapper.source.records, 1):
        for entity in mapper.map(record).values():
            entity.context = mapping.get_proxy_context()
            entity.context['mutable'] = True
            if entity.schema.is_a('Thing'):
                entity.add('proof', mapping.table_id)
            entity = collection.ns.apply(entity)
            entity = remove_checksums(entity)
            writer.put(entity, fragment=idx, origin=origin)
            yield entity
    writer.flush()


def flush_mapping(stage, collection, mapping_id, sync=False):
    """Delete entities loaded by a mapping"""
    log.debug("Flushing entities for mapping: %s", mapping_id)
    origin = mapping_origin(mapping_id)
    delete_entities(collection.id, origin=origin, sync=True)
    drop_aggregator(collection, origin=origin)
    collection.touch()
    db.session.commit()
    update_collection(collection)
